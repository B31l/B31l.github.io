---
title: "딥러닝 이론: 순전파 & 역전파"
categories: [AI-딥러닝 이론]
mathjax: true
---

* content
{:toc}
# 에포크

```python
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        
        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```



## 순전파(Forward propagation)

신경망의 **계층**(Layer)을 구성하는 모든 **노드**(Node)는 각각의 **가중치**(Weight)와 **편향**(bias)을 가지고 있다.

순전파(Forward propagation) 과정에서는 데이터**X**가 입력되면 가중치**W** 및 편향**b**을 사용해 입력층에서 출력층 방향으로 이동하며 예측값을 계산하고, 손실 함수를 사용해 예측값과 실제값의 오차**Loss**를 구한다.

```python
pred = model(x_batch)
loss = loss_fn(pred, y_batch)
```

## 역전파(Back propagation)

순전파 과정에서 구한 오차를 최소화하기 위해 각 노드의 가중치와 편향을 수정해야 한다.

역전파(Back propagation) 과정에서는 연쇄 법칙을 사용해 오차**Loss**를 가중치**W** 또는 편향**b**으로 편미분한다.

```python
loss.backward()
optimizer.step()
```



## 자동 미분



---

# 📌REF

-   **Machine Learning with PyTorch and Scikit-Learn**

