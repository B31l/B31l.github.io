---
title: "활성화 함수(Activation Function)"
categories: [DeepLearning]
tags: Python
mathjax: true
---

* content
{:toc}
# 개념

신경망에서 은닉층의 수가 많아지면 더 적은 매개변수와 연산으로도 동일한 성능의 모델을 구현할 수 있다. 즉  망이 깊어질수록 신경망의 효율이 증가한다.

하지만 각 층(Layer)의 출력값을 다음 층의 입력값으로 넘겨주는 과정에서 데이터를 선형으로 사용한다면 문제가 발생한다.

겉으로는 층을 많이 쌓은 것처럼 보일지라도, 연산을 통해 하나의 층으로 나타낼 수 있기에 실제로는 망이 깊어지지 않기 때문이다.

따라서 비선형 활성화 함수를 적용해 각 층의 출력값을 비선형으로 바꿀 필요가 있다.

---

# 종류

활성화 함수는 모양에 따라 **선형**과 **비선형**으로 나눌 수 있다.

앞서 언급했듯이 은닉층과 출력층에서 선형 활성화 함수를 사용하는 것은 비효율적이다.

다음 비선형 활성화 함수가 주로 사용되며, 선형 활성화 함수의 한계를 해결한다.

-   **로지스틱 시그모이드**(Logistic Sigmoid)
-   **소프트맥스**(Softmax)
-   **하이퍼볼릭 탄젠트**(Hyperbolic tangent)
-   **렐루**(ReLU)

## 로지스틱 시그모이드(Logistic Sigmoid)

$$σ(z)=\frac{1}{1+e^{-z}}$$

대표적인 시그모이드(Sigmoid) 함수로, 일반적으로 시그모이드 함수라고 하면 이 함수를 가리킨다.

로지스틱 시그모이드 함수는 (0, 1) 범위의 출력값을 가지며, **이진 분류**(Binary Classification)에 사용된다.

신경망에서는 주로 중간층에서 활성화 함수로 사용된다.

음수 값의 출력값은 0에 매우 가깝기 때문에, 많은 음수 값이 입력층에 들어올 경우 신경망 학습이 매우 더디게 진행되는 단점이 있다. 

또한 출력값의 범위가 매우 좁기 때문에, 신경망의 은닉층이 많아질수록 **기울기 소실**(Vanishing Gradient)이 발생하기 쉽다.

파이토치에서는 다음과 같이 사용한다.

```python
torch.sigmoid(텐서)
```

## 소프트맥스(Softmax)

$p_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_j}}$

$j = 1,2, \dots ,K$

시그모이드 함수 중 하나로, argmax 함수의 부드러운 버전이라고 생각하면 된다.

소프트맥스 함수는 [0, 1] 범위의 출력값을 가지며, **다중 분류**(Multiclass Classification)에 사용된다.

신경망에서는 주로 출력층에서 활성화 함수로 사용된다.

단일 클래스의 인덱스 대신 각 클래스에 해당할 확률을 반환하며, 이 때 확률의 총합은 1이다.

파이토치에서는 다음과 같이 사용한다.

```python
torch.softmax(텐서, dim=0)
```

## 하이퍼볼릭 탄젠트(Hyperbolic tangent)

$$σ(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$

하이퍼볼릭 탄젠트 함수는 (-1, 1) 범위의 출력값을 가진다.

하이퍼볼릭 탄젠트 함수는 로지스틱 시그모이드 함수보다 음수를 처리하기 쉽기에 기울기 소실 문제를 어느 정도 방지할 수 있다.

로지스틱 함수와 하이퍼볼릭 탄젠트를 비교한 그래프는 다음과 같다.

![](https://i.imgur.com/4zEB4RR.png)

파이토치에서는 다음과 같이 사용한다.

```python
torch.tanh(텐서)
```

## 렐루(ReLU)

$$σ(z)=\begin {cases}0~~~~z<0\\z~~~~z>1\end {cases}$$

렐루 함수는 음수값의 출력값을 모두 0으로 처리한다.

렐루 함수의 도함수는 항상 1이므로 기울기 소실 문제를 방지할 수 있다.

복잡한 모델 훈련 시 오차가 빠르게 감소하는 탁월한 성능을 보여주므로 심층 신경망 구현에 적합하다.

![](https://i.imgur.com/We3GSJ4.png)

파이토치에서는 다음과 같이 사용한다.

```python
torch.relu(텐서)
```

---
