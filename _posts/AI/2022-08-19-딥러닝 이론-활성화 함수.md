---
title: "딥러닝 이론: 활성화 함수"
categories: [AI-딥러닝 이론]
mathjax: true
---

* content
{:toc}
# 활성화 함수<sub>Activation function</sub>

신경망에서 은닉층의 수가 많아지면 더 적은 매개변수와 연산으로도 동일한 성능의 모델을 구현할 수 있다. 즉, 망이 깊어질수록 신경망의 효율이 증가한다.

하지만 각 **계층**<sub>Layer</sub>의 출력값을 다음 층의 입력값으로 넘겨주는 과정에서 데이터를 선형으로 사용한다면 문제가 발생한다.

겉으로는 층을 많이 쌓은 것처럼 보일지라도, 연산을 통해 하나의 층으로 나타낼 수 있기에 실제로는 망이 깊어지지 않기 때문이다.

따라서 각 층에 비선형 활성화 함수를 적용해 출력값을 비선형으로 바꿀 필요가 있다.

---

# 종류

활성화 함수는 모양에 따라 **선형**과 **비선형**으로 나눌 수 있다.

앞서 언급했듯이 은닉층과 출력층에서 선형 활성화 함수를 사용하는 것은 비효율적이다.

다음 비선형 활성화 함수가 주로 사용되며, 선형 활성화 함수의 한계를 해결한다.

-   **로지스틱 시그모이드**<sub>Logistic sigmoid</sub>
-   **소프트맥스**<sub>Softmax</sub>
-   **하이퍼볼릭 탄젠트**<sub>Hyperbolic tangent</sub>
-   **렐루**<sub>ReLU</sub>

## 로지스틱 시그모이드<sub>Logistic sigmoid</sub>

$$σ(z)=\frac{1}{1+e^{-z}}$$

| 🧶                      |
| ---------------------- |
| `torch.sigmoid(input)` |

로지스틱 시그모이드 함수는 대표적인 **시그모이드**<sub>Sigmoid</sub> 함수로, (0, 1) 범위의 출력값을 가진다.

**이진 분류**<sub>Binary classification</sub>에 적합하기에 주로 이진 분류 모델의 출력층에서 활성화 함수로 사용된다.

음수 값의 출력값은 0에 매우 가깝기 때문에, 많은 음수 값이 입력층에 들어올 경우 신경망 학습이 매우 더디게 진행되는 단점이 있다. 

또한 출력값의 범위가 매우 좁기 때문에, 신경망의 은닉층이 많아질수록 **기울기 소실**<sub>Vanishing gradient</sub>이 발생하기 쉽다.

## 소프트맥스<sub>Softmax</sub>

$p_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_j}}$

$j = 1,2, \dots ,K$

시그모이드 함수 중 하나로, **argmax** 함수의 부드러운 버전이라고 할 수 있다.

소프트맥스 함수는 로지스틱 시그모이드 함수와 마찬가지로 [0, 1] 범위의 출력값을 가진다.

차이점으로 단일 클래스의 인덱스 대신 **각 클래스에 해당할 확률**을 반환하며, 이 때 확률의 총합은 1이다.

**다중 분류**<sub>Multiclass classification</sub>에 적합하기에 주로 다중 분류 모델의 출력층에서 활성화 함수로 사용된다.

## 하이퍼볼릭 탄젠트<sub>Hyperbolic tangent</sub>

| 🧶                   |
| ------------------- |
| `torch.tanh(input)` |

$$σ(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$

하이퍼볼릭 탄젠트 함수는 (-1, 1) 범위의 출력값을 가진다.

시그모이드 함수에 비해 음수 값을 처리하기 쉽기 때문에 기울기 소실을 어느 정도 방지할 수 있다.

로지스틱 함수와 하이퍼볼릭 탄젠트를 비교한 그래프는 다음과 같다.

![](https://i.imgur.com/4zEB4RR.png)

## 렐루<sub>ReLU</sub>

| 🧶                        |
| ------------------------ |
| ````torch.relu(텐서)```` |

$$σ(z)=\begin {cases}0~~~~z<0\\z~~~~z>1\end {cases}$$

렐루 함수는 음수값의 출력값을 모두 0으로 처리한다.

렐루 함수는 단순한 모양의 함수이기에 속도가 매우 빠르며, 도함수가 항상 1이므로 기울기 소실을 방지할 수 있다.

복잡한 모델 훈련 시 오차가 빠르게 감소하는 탁월한 성능을 보여주므로 심층 신경망 구현에 적합하다.

이러한 장점들 덕분에 은닉층의 활성화함수로 많이 사용된다.

---

# 📌REF

-   **Machine Learning with PyTorch and Scikit-Learn**

