---

layout: post
meta: "Pytorch12"
title: "Parallelizing Neural Network Training with PyTorch"
categories: DL
tags: Python
comments: true
mathjax: true
---



* content
{:toc}
# 개요

[소스코드](https://colab.research.google.com/drive/1F4KBc63XJVn3cMfQlbZZi5thbPJnHhIZ)



# 맛보기

## 데이터 적재

본격적으로 신경망을 학습하기 전에, 선형 회귀 문제를 해결하는 간단한 맛보기 모델을 제작해 본다.

넘파이 어레이를 사용해 샘플(X_train) 및 레이블(y_train)을 생성한다.

```python
X_train = np.arange(10, dtype="float32").reshape((10, 1))
y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype="float32")
```

X_train에 대해 표준화 과정을 거친 다음, `torch.from_numpy`를 사용해 텐서로 변환한다.

```python
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)
X_train_norm = torch.from_numpy(X_train_norm)
y_train = torch.from_numpy(y_train)
```

데이터 적재와 순회를 위해 TensorDataset(`torch.utils.data.TensorDataset`)와 DataLoader(`torch.utils.data.DataLoader`)를 사용한다.

TensorDataset은 Dataset(`torch.utils.data.Dataset`)의 하위 클래스로, 샘플 X와 레이블 Y를 묶어 놓는 컨테이너이다.

DataLoader은 TensorDataset을 순환 가능(Iterable)하게 만든다. 즉, for 루프를 사용해 쉽게 접근할 수 있게 된다. 조정 가능한 하이퍼파라미터는 다음과 같다.

- `batch_size`

  **배치**의 크기를 지정한다. 현재 데이터셋에는 10개의 데이터가 있고, batch_size를 1로 지정하면 10/1 = 10번의 반복이 발생한다.

- `shuffle`

  데이터를 섞어서 사용할지 결정한다.

```python
from torch.utils.data import TensorDataset, DataLoader

train_ds = TensorDataset(X_train_norm, y_train)
train_dl = DataLoader(train_ds, batch_size=1, shuffle=True)
```

> *Note*
>
> DataLoader 클래스의 조정 가능한 모든 하이퍼파라미터는 다음과 같다.
>
> ```
> DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
>            batch_sampler=None, num_workers=0, collate_fn=None,
>            pin_memory=False, drop_last=False, timeout=0,
>            worker_init_fn=None, *, prefetch_factor=2,
>            persistent_workers=False)
> ```
>
> 출처: https://pytorch.org/docs/stable/data.html

## 모델 훈련

선형 회귀 모델을 사용하며, 이는 다음과 같은 선형 방정식으로 나타낼 수 있다.

$$ z = wx + b $$ 

가중치(w) 및 편향(b)에 주목해야 한다. 이는 각각 선형 방정식의 기울기와 절편에 대응하며, 그래프의 모양을 결정한다. 가중치와 편향은 임의의 값에서 출발하고, 확률적 경사 하강법을 사용해 비용(손실 함수의 결과)이 최소화되는 방향으로 이동한다.

파이토치에서는 **자동 미분**(Autograd)을 지원하므로, 경사 하강법 알고리즘을 직접 구현하지 않아도 된다. 자동 미분은 텐서를 생성할 때 `requires_grad=True`로 설정하거나, 나중에 `requires_grad_()`로 활성화한다.

```python
torch.manual_seed(1)

# 가중치
weight = torch.randn(1)
weight.requires_grad_()                     # 자동 미분 활성화

# 편향
bias = torch.zeros(1, requires_grad=True)   # 자동 미분 활성화
```

모델은 선형 회귀 모델, 손실 함수는 **MSE**를 사용한다.

```python
# 선형 회귀 모델
def model(xb):
    return xb @ weight + bias

# 손실 함수: MSE | 평균 제곱 오차
def loss_fn(input, target):
    return (input-target).pow(2).mean()
```

모델, 가중치, 편향, 손실 함수의 설정이 끝났다면, 학습률과 에포크를 설정하고 모델 훈련을 시행한다.

```python
learning_rate = 0.001
num_epochs = 200
log_epochs = 10
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred = model(x_batch)           # 예측값
        loss = loss_fn(pred, y_batch)   # 오차
        loss.backward()                 # 변화도 계산
    with torch.no_grad():
        # 계산한 변화도를 사용해 파라미터 업데이트
        weight -= weight.grad * learning_rate
        bias -= bias.grad * learning_rate
        # 변화도 초기화
        weight.grad.zero_()
        bias.grad.zero_()
    if epoch % log_epochs == 0:
        print(f"Epoch {epoch} Loss {loss.item():.4f}")
```

각 학습마다 다음 작업을 수행한다.

- 모든 데이터셋에 대해

  - DataLoader에서 x_batch 및 y_batch를 가져온다.
  - 모델(선형 방정식)의 인자로 x_batch를 사용해 예측값을 구한다.
  - 예측값과 실제값(y_batch)의 오차를 구한다.
  - `backward()`는 자동 미분이 활성화된 모든 텐서들(weight, bias)에 대한 **gradient**(변화도)을 추적하고, 이를 각 텐서의 `grad` 속성에 저장한다.

- `no_grad()`를 **with**문과 같이 사용하면 Pytorch는 autograd engine을 끄고, 변화도 추적을 중지한다. 이 동안 메모리 사용량을 줄이고, 연산 속도를 높일 수 있다.
  - 계산한 변화도를 사용해 가중치와 편향을 갱신한다.
  - `grad_zero_()`를 사용해 **변화도를 초기화한다.** 만약 초기화하지 않는다면 값은 그대로 남아 있으며, 다음 학습에서 변화도를 추적할 때 `grad`에 값이 축적되는 일이 발생한다.

- 10번마다 오차를 출력한다.

# 신경망

`torch.nn`이 제공하는 클래스 및 함수를 사용하면 앞서 구현한 내용을 보다 간단하게 작성할 수 있다.

```python
import torch.nn as nn
loss_fn = nn.MSELoss(reduction="mean")  # 평균 제곱 오차
input_size = 1                          # 입력층: 1
output_size = 1                         # 출력층: 1
model = nn.Linear(input_size, output_size)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   # SGD optimizer
```

 

```python
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred = model(x_batch)[:, 0]     # 예측값
        loss = loss_fn(pred, y_batch)   # 오차
        loss.backward()                 # 역전파: 기울기 계산
        optimizer.step()                # 계산한 기울기를 사용해 파라미터 초기화
        optimizer.zero_grad()           # 기울기 초기화
    if epoch % log_epochs == 0:
        print(f"Epoch {epoch} Loss {loss.item():.4f}")
```
