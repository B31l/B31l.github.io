---
title: "ì„ í˜• íšŒê·€ ë° ì„ í˜• ë¶„ë¥˜"
categories: [DeepLearning]
tags: Python
mathjax: true
---

* content
{:toc}
# ì„œë¡ 

ì „ì²´ ì½”ë“œëŠ” [Colab](https://colab.research.google.com/drive/1F4KBc63XJVn3cMfQlbZZi5thbPJnHhIZ)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

# ì„ í˜• íšŒê·€

## ë°ì´í„° ì ì¬í•˜ê¸°

ë³¸ê²©ì ìœ¼ë¡œ ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê¸° ì „ì—, ì„ í˜• íšŒê·€ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê°„ë‹¨í•œ ë§›ë³´ê¸° ëª¨ë¸ì„ ì œì‘í•´ ë³¸ë‹¤.

ë„˜íŒŒì´ ì–´ë ˆì´ë¥¼ ì‚¬ìš©í•´ ìƒ˜í”Œ(X_train) ë° ë ˆì´ë¸”(y_train)ì„ ìƒì„±í•œë‹¤.

```python
X_train = np.arange(10, dtype="float32").reshape((10, 1))
y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0], dtype="float32")
```

ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![](https://i.imgur.com/QcodNwb.png)

X_trainì„ í‘œì¤€í™”í•˜ê³ , `torch.from_numpy`ë¥¼ ì‚¬ìš©í•´ ìƒ˜í”Œê³¼ ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜í•œë‹¤.

```python
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)
X_train_norm = torch.from_numpy(X_train_norm)
y_train = torch.from_numpy(y_train)
```

ë°ì´í„° ì ì¬ì™€ ìˆœíšŒë¥¼ ìœ„í•´ TensorDataset(`torch.utils.data.TensorDataset`)ì™€ DataLoader(`torch.utils.data.DataLoader`)ë¥¼ ì‚¬ìš©í•œë‹¤.

TensorDatasetì€ Dataset(`torch.utils.data.Dataset`)ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¡œ, ìƒ˜í”Œ Xì™€ ë ˆì´ë¸” Yë¥¼ ë¬¶ì–´ ë†“ëŠ” ì»¨í…Œì´ë„ˆì´ë‹¤.

DataLoaderì€ TensorDatasetì„ ìˆœí™˜ ê°€ëŠ¥(Iterable)í•˜ê²Œ ë§Œë“ ë‹¤. ì¦‰, for ë£¨í”„ë¥¼ ì‚¬ìš©í•´ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì¡°ì • ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- `batch_size`

  **ë°°ì¹˜**ì˜ í¬ê¸°ë¥¼ ì§€ì •í•œë‹¤. í˜„ì¬ ë°ì´í„°ì…‹ì—ëŠ” 10ê°œì˜ ë°ì´í„°ê°€ ìˆê³ , batch_sizeë¥¼ 1ë¡œ ì§€ì •í•˜ë©´ 10/1 = 10ë²ˆì˜ ë°˜ë³µì´ ë°œìƒí•œë‹¤.

- `shuffle`

  ë°ì´í„°ë¥¼ ì„ì–´ì„œ ì‚¬ìš©í• ì§€ ê²°ì •í•œë‹¤.

```python
from torch.utils.data import TensorDataset, DataLoader

train_ds = TensorDataset(X_train_norm, y_train)
train_dl = DataLoader(train_ds, batch_size=1, shuffle=True)
```

> *Note*
>
> DataLoader í´ë˜ìŠ¤ì˜ ì¡°ì • ê°€ëŠ¥í•œ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
>
> ```
> DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
>         batch_sampler=None, num_workers=0, collate_fn=None,
>         pin_memory=False, drop_last=False, timeout=0,
>         worker_init_fn=None, *, prefetch_factor=2,
>         persistent_workers=False)
> ```
>
> ì¶œì²˜: <https://pytorch.org/docs/stable/data.html>

## ëª¨ë¸ í›ˆë ¨í•˜ê¸°

ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ë°©ì •ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ z = wx + b $$ 

ê°€ì¤‘ì¹˜(w) ë° í¸í–¥(b)ì— ì£¼ëª©í•´ì•¼ í•œë‹¤. ì´ëŠ” ê°ê° ì„ í˜• ë°©ì •ì‹ì˜ ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì— ëŒ€ì‘í•˜ë©°, ê·¸ë˜í”„ì˜ ëª¨ì–‘ì„ ê²°ì •í•œë‹¤. ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì€ ì„ì˜ì˜ ê°’ì—ì„œ ì¶œë°œí•˜ê³ , í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•´ ë¹„ìš©(ì†ì‹¤ í•¨ìˆ˜ì˜ ê²°ê³¼)ì´ ìµœì†Œí™”ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤.

íŒŒì´í† ì¹˜ì—ì„œëŠ” **ìë™ ë¯¸ë¶„**(Autograd)ì„ ì§€ì›í•˜ë¯€ë¡œ, ê²½ì‚¬ í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. ìë™ ë¯¸ë¶„ì€ í…ì„œë¥¼ ìƒì„±í•  ë•Œ `requires_grad=True`ë¡œ ì„¤ì •í•˜ê±°ë‚˜, ë‚˜ì¤‘ì— `requires_grad_()`ë¡œ í™œì„±í™”í•œë‹¤.

```python
torch.manual_seed(1)

# ê°€ì¤‘ì¹˜
weight = torch.randn(1)
weight.requires_grad_()                     # ìë™ ë¯¸ë¶„ í™œì„±í™”

# í¸í–¥
bias = torch.zeros(1, requires_grad=True)   # ìë™ ë¯¸ë¶„ í™œì„±í™”
```

ëª¨ë¸ì€ ì„ í˜• íšŒê·€ ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜ëŠ” **MSE**(í‰ê·  ì œê³± ì˜¤ì°¨)ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
# ì„ í˜• íšŒê·€ ëª¨ë¸
def model(xb):
    return xb @ weight + bias

# ì†ì‹¤ í•¨ìˆ˜: MSE | í‰ê·  ì œê³± ì˜¤ì°¨
def loss_fn(input, target):
    return (input-target).pow(2).mean()
```

ëª¨ë¸, ê°€ì¤‘ì¹˜, í¸í–¥, ì†ì‹¤ í•¨ìˆ˜ì˜ ì„¤ì •ì´ ëë‚¬ë‹¤ë©´, í•™ìŠµë¥ ê³¼ ì—í¬í¬ë¥¼ ì„¤ì •í•˜ê³  ëª¨ë¸ í›ˆë ¨ì„ ì‹œí–‰í•œë‹¤.

```python
learning_rate = 0.001
num_epochs = 200
log_epochs = 10
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred = model(x_batch)           # ì˜ˆì¸¡ê°’
        loss = loss_fn(pred, y_batch)   # ì˜¤ì°¨
        loss.backward()                 # ë³€í™”ë„ ê³„ì‚°
    with torch.no_grad():
        # ê³„ì‚°í•œ ë³€í™”ë„ë¥¼ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        weight -= weight.grad * learning_rate
        bias -= bias.grad * learning_rate
        # ë³€í™”ë„ ì´ˆê¸°í™”
        weight.grad.zero_()
        bias.grad.zero_()
    if epoch % log_epochs == 0:
        print(f"Epoch {epoch} Loss {loss.item():.4f}")
```

ê° í•™ìŠµë§ˆë‹¤ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

- ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´

  - DataLoaderì—ì„œ x_batch ë° y_batchë¥¼ ê°€ì ¸ì˜¨ë‹¤.
  - ëª¨ë¸(ì„ í˜• ë°©ì •ì‹)ì˜ ì¸ìë¡œ x_batchë¥¼ ì‚¬ìš©í•´ ì˜ˆì¸¡ê°’ì„ êµ¬í•œë‹¤.
  - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’(y_batch)ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•œë‹¤.
  - `backward()`ëŠ” ìë™ ë¯¸ë¶„ì´ í™œì„±í™”ëœ ëª¨ë“  í…ì„œë“¤(weight, bias)ì— ëŒ€í•œ **gradient**(ë³€í™”ë„)ì„ ì¶”ì í•˜ê³ , ì´ë¥¼ ê° í…ì„œì˜ `grad` ì†ì„±ì— ì €ì¥í•œë‹¤.
- `no_grad()`ë¥¼ **with**ë¬¸ê³¼ ê°™ì´ ì‚¬ìš©í•˜ë©´ PytorchëŠ” autograd engineì„ ë„ê³ , ë³€í™”ë„ ì¶”ì ì„ ì¤‘ì§€í•œë‹¤. ì´ ë™ì•ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³ , ì—°ì‚° ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤.
  - ê³„ì‚°í•œ ë³€í™”ë„ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê°±ì‹ í•œë‹¤.
  - `grad_zero_()`ë¥¼ ì‚¬ìš©í•´ **ë³€í™”ë„ë¥¼ ì´ˆê¸°í™”í•œë‹¤.** ë§Œì•½ ì´ˆê¸°í™”í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ê°’ì€ ê·¸ëŒ€ë¡œ ë‚¨ì•„ ìˆìœ¼ë©°, ë‹¤ìŒ í•™ìŠµì—ì„œ ë³€í™”ë„ë¥¼ ì¶”ì í•  ë•Œ `grad`ì— ê°’ì´ ì¶•ì ë˜ëŠ” ì¼ì´ ë°œìƒí•œë‹¤.
- 10ë²ˆë§ˆë‹¤ ì˜¤ì°¨ë¥¼ ì¶œë ¥í•œë‹¤.

í›ˆë ¨ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![](https://i.imgur.com/uwov6fo.png)

## torch.nn í™œìš©í•˜ê¸°

PyTorchê°€ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì•ì„œ êµ¬í˜„í•œ ë‚´ìš©ì„ ë³´ë‹¤ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.

```python
import torch.nn as nn
```

`torch.nn` ì€ ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œê³µí•˜ë©°, ì„ í˜• íšŒê·€ ëª¨ë¸(`torch.nn.Linear`) ë° MSE ì†ì‹¤ í•¨ìˆ˜(`torch.nn.MSELoss`)ë¥¼ ì‚¬ìš©í•œë‹¤. ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ì¡°ì • ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- `in_features`

  ì…ë ¥ì˜ í¬ê¸°ë¥¼ ì •ì˜í•œë‹¤.

- `out_features`

  ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ì •ì˜í•œë‹¤.

- `bias`

  Falseë¡œ ì„¤ì •í•˜ë©´, ì‹ ê²½ë§ì˜ ë ˆì´ì–´ëŠ” í¸í–¥ì„ ì¶”ê°€ë¡œ í•™ìŠµí•˜ì§€ ì•ŠëŠ”ë‹¤. (Default: True)

`torch.optim`ì€ ì˜µí‹°ë§ˆì´ì €(ìµœì í™”) ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•˜ë©°, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• ì˜µí‹°ë§ˆì´ì €(`torch.optim.SGD`) ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
input_size = 1
output_size = 1
model = nn.Linear(input_size, output_size)
loss_fn = nn.MSELoss(reduction="mean")
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```

ë™ì¼í•œ í•™ìŠµë¥ ê³¼ ì—í¬í¬ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ í›ˆë ¨ì„ ì‹œí–‰í•œë‹¤.

```python
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred = model(x_batch)[:, 0]     # ì˜ˆì¸¡ê°’
        loss = loss_fn(pred, y_batch)   # ì˜¤ì°¨
        loss.backward()                 # ë³€í™”ë„ ê³„ì‚°
        optimizer.step()                # ê³„ì‚°í•œ ë³€í™”ë„ë¥¼ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        optimizer.zero_grad()           # ë³€í™”ë„ ì´ˆê¸°í™”
    if epoch % log_epochs == 0:
        print(f"Epoch {epoch} Loss {loss.item():.4f}")
```

ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•´ **ì—­ì „íŒŒ** ê³¼ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

```python
loss.backward()                 # ë³€í™”ë„ ê³„ì‚°
optimizer.step()                # ê³„ì‚°í•œ ë³€í™”ë„ë¥¼ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
optimizer.zero_grad()           # ë³€í™”ë„ ì´ˆê¸°í™”
```

# ì„ í˜• ë¶„ë¥˜

## ì‚¬ì „ ì‘ì—…í•˜ê¸°

NNì„ êµ¬í˜„í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•´ ë¶“ê½ƒ ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•œë‹¤. `sklearn.datasets`ì„ ì‚¬ìš©í•´ ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¤ê³ , `train_test_split`ì„ ì‚¬ìš©í•´ í›ˆë ¨ì…‹ê³¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris["data"]
y = iris["target"]
X_train, X_test, y_train, y_test = train_test_split(
    # í›ˆë ¨ì…‹:í…ŒìŠ¤íŠ¸ì…‹ = 2:1
    X, y, test_size=1./3, random_state=1    
)
```

Xë¥¼ í‘œì¤€í™”í•˜ê³  ìƒ˜í”Œê³¼ ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜í•œ í›„, ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ 2ì¸ DataLoaderë¥¼ ìƒì„±í•œë‹¤.

```python
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)
X_train_norm = torch.from_numpy(X_train_norm).float()
y_train = torch.from_numpy(y_train)
train_ds = TensorDataset(X_train_norm, y_train)
torch.manual_seed(1)
batch_size = 2
train_dl = DataLoader(train_ds, batch_size, shuffle=True)   # ë°°ì¹˜ ì‚¬ì´ì¦ˆ 2ì¸ DataLoader
```

## ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°

ì•ì„œ í•™ìŠµí•œ ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ê°€ì§€ëŠ”, PyTorchê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ëª¨ë¸ ì¤‘ í•˜ë‚˜ì´ë‹¤.

í•˜ì§€ë§Œ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶”ê°€ë¡œ ì€ë‹‰ì¸µì„ í¬í•¨ì‹œì¼œì•¼ í•˜ë©°, ê° ë‰´ëŸ°ì— ëŒ€í•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•˜ë‹¤.

ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì€ `torch.nn.Module`ì„ ìƒì†í•´ ê°„ë‹¨í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. Model í´ë˜ìŠ¤ëŠ” ë‘ ê°œì˜ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ì€ë‹‰ì¸µìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì€ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì€ë‹‰ì¸µ ë‰´ëŸ°ì„ ìƒì„±í•˜ë©°, ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µì€ ì€ë‹‰ì¸µ ë‰´ëŸ°ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ì„ ìƒì„±í•œë‹¤.

```python
class Model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ë¥˜
        self.layer1 = nn.Linear(input_size, hidden_size)    # ì€ë‹‰ì¸µ: ì…ë ¥ì¸µ ë‰´ëŸ° -> ì€ë‹‰ì¸µ ë‰´ëŸ°
        self.layer2 = nn.Linear(hidden_size, output_size)   # ì¶œë ¥ì¸µ: ì€ë‹‰ì¸µ ë‰´ëŸ° -> ì¶œë ¥ì¸µ ë‰´ëŸ°
    ...
```

ìˆœì „íŒŒë¥¼ ìˆ˜í–‰í•˜ëŠ” `forward` ë©”ì„œë“œë¥¼ ì •ì˜í•œë‹¤. ì€ë‹‰ì¸µê³¼ ì¶œë ¥ì¸µì„ ì—°ê²°í•˜ê³ , ê° ì¸µ ì‚¬ì´ì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ì¶”ê°€í•œë‹¤.

í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì‹œê·¸ëª¨ì´ë“œ(`torch.nn.Sigmoid`) ë° ì†Œí”„íŠ¸ë§¥ìŠ¤(`torch.nn.Softmax`)ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
    ...
    def forward(self, x):
        x = self.layer1(x)
        x = nn.Sigmoid()(x)         # ì‹œê·¸ëª¨ì´ë“œ
        x = self.layer2(x)
        x = nn.Softmax(dim=1)(x)    # ì†Œí”„íŠ¸ë§¥ìŠ¤
        return x
```

> *Note*
>
> ì‹¤ì œ ì‹ ê²½ë§ì˜ ì¸µ ê°œìˆ˜ë¥¼ ì…€ ë•Œ ì…ë ¥ì¸µì€ ìƒëµí•œë‹¤.

> *Note*
>
> ê° ì¸µì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì¸µì— ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ëœë‹¤. ì¸µì— ìŒìˆ˜ê°€ ë§ì´ ì…ë ¥ë  ê²½ìš° ì‹ ê²½ë§ í•™ìŠµì´ ë§¤ìš° ë”ë””ê²Œ ì§„í–‰ë˜ë©°, ì…ë ¥ ê°’ì´ ìµœì¢… ë ˆì´ì–´ì—ì„œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì‘ì•„ì§ˆ ìˆ˜(**Vanishing Gradient Problem**) ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ìŒìˆ˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

ì´ë²ˆ ë¶„ë¥˜ ëª¨ë¸ì˜ ëª©ì ì€, ë¶“ê½ƒ ë°ì´í„°ì…‹ì˜ 4ê°œì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ë¶“ê½ƒì„ 3ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì…ë ¥ì¸µì˜ í¬ê¸°(ë‰´ëŸ°ì˜ ìˆ˜)ëŠ” 4, ì¶œë ¥ì¸µì˜ í¬ê¸°ëŠ” 3ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ì€ë‹‰ì¸µì˜ í¬ê¸°ëŠ” 16ìœ¼ë¡œ ì„¤ì •í•œë‹¤.

ì†ì‹¤ í•¨ìˆ˜ë¡œ **CrossEntropyLoss**, ì˜µí‹°ë§ˆì´ì €ë¡œ **Adam**ì„ ì‚¬ìš©í•œë‹¤. ì´ ë‘˜ì€ ë‹¤ì¤‘ ë¶„ë¥˜ì— í”íˆ ì‚¬ìš©ëœë‹¤.

```python
input_size = X_train_norm.shape[1]  # 4
hidden_size = 16
output_size = 3
model = Model(input_size, hidden_size, output_size)
loss_fn = nn.CrossEntropyLoss()                                     # CrossEntropyLoss
learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam
```

ì—í¬í¬ë¥¼ ì„¤ì •í•˜ê³  ëª¨ë¸ í›ˆë ¨ì„ ì‹œí–‰í•œë‹¤.

ì†ì‹¤ í•¨ìˆ˜ê°€ `backward` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ì„œ ì—­ì „íŒŒê°€ ì‹œì‘ëœë‹¤. SGD ì˜µí‹°ë§ˆì´ì €ëŠ” ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë©° ì—°ì‡„ ë¯¸ë¶„ ë²•ì¹™ì„ í†µí•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ë³€í™”ë„ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

```python
num_epochs = 100
loss_hist = [0] * num_epochs
accuracy_hist = [0] * num_epochs
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        
        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # update hist
        loss_hist[epoch] += loss.item() * y_batch.size(0)
        is_correct = (torch.argmax(pred, dim=1) == y_batch).float() # True:1, False:0
        accuracy_hist[epoch] += is_correct.mean()
    loss_hist[epoch] /= len(train_dl.dataset)                       # ê° ì—í¬í¬ì˜ ì˜¤ì°¨
    accuracy_hist[epoch] /= len(train_dl.dataset)                   # ê° ì—í¬í¬ì˜ ì •ë‹µë¥ 
    accuracy_hist[epoch] *= batch_size
```

![](https://i.imgur.com/3tnFeiK.png)

í•™ìŠµí•˜ëŠ” ë™ì•ˆ ì˜¤ì°¨ëŠ” ê¾¸ì¤€íˆ ê°ì†Œí–ˆìœ¼ë©°, ì •í™•ë„ëŠ” ì•½ 30ë²ˆì§¸ ì—í¬í¬ì—ì„œ ê¸‰ì¦í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

í›ˆë ¨ì´ ëë‚œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•´ í‰ê°€í•œë‹¤.

```python
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm).float()
y_test = torch.from_numpy(y_test)
pred_test = model(X_test_norm)
correct = (torch.argmax(pred_test, dim=1) == y_test).float()
accuracy = correct.mean()
print(f"Test Acc.: {accuracy:.4f}")
```

```
Test Acc.: 0.9800
```

---

# ğŸ“ŒREF

